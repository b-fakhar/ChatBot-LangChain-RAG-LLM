{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQI-itr5h8b4"
      },
      "outputs": [],
      "source": [
        "# ! pip install streamlit\n",
        "# ! pip install python-dotenv\n",
        "# ! pip install langchain\n",
        "# ! pip install -U langchain-community\n",
        "# ! pip install pypdf\n",
        "# ! pip install InstructorEmbedding\n",
        "# ! pip install sentence-transformers==2.2.2\n",
        "# ! pip install faiss-gpu\n",
        "# ! pip install faiss-cpu\n",
        "# ! pip install PyPDF2\n",
        "# ! pip install ctransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5ZwTt04fF8H",
        "outputId": "07bfd30d-4e0c-40f9-ea4e-bb36ee306eb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "import streamlit                      as st\n",
        "\n",
        "from dotenv                           import load_dotenv\n",
        "from PyPDF2                           import PdfReader\n",
        "from langchain_text_splitters         import CharacterTextSplitter\n",
        "from langchain_community.embeddings   import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.memory                 import ConversationBufferMemory\n",
        "from langchain.chains                 import ConversationalRetrievalChain\n",
        "from langchain_community.llms         import HuggingFaceHub\n",
        "from htmlTemplates                    import css, bot_template, user_template\n",
        "from langchain_community.llms         import CTransformers\n",
        "\n",
        "\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vectorstore(text_chunks):\n",
        "    # embeddings = OpenAIEmbeddings()\n",
        "    # embeddings  = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-base\")\n",
        "    embeddings  = HuggingFaceInstructEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "\n",
        "def get_conversation_chain(vectorstore):\n",
        "    # llm = ChatOpenAI()\n",
        "    # llm = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\", model_kwargs={\"temperature\":0.5, \"max_length\":512})\n",
        "    llm = CTransformers(model=\"llama-2-7b-chat.ggmlv3.q4_0.bin\",model_type=\"llama\",\n",
        "                    config={'max_new_tokens':128,'temperature':0.01})\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True)\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "    response = st.session_state.conversation({'question': user_question})\n",
        "    st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.write(user_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(bot_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "st.set_page_config(page_title=\"Chat with PDFs!\",\n",
        "                    page_icon=\":speech_balloon:\")\n",
        "st.write(css, unsafe_allow_html=True)\n",
        "\n",
        "if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = None\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = None\n",
        "\n",
        "st.header(\"Chat with PDFs! :speech_balloon:\")\n",
        "user_question = st.text_input(\"Ask a question about your uploaded documents:\")\n",
        "if user_question:\n",
        "    with st.spinner(\"Processing\"):\n",
        "        handle_userinput(user_question)\n",
        "\n",
        "with st.sidebar:\n",
        "    st.subheader(\"Your documents\")\n",
        "    pdf_docs = st.file_uploader(\n",
        "        \"Upload your PDFs here and click on 'Enter'\", accept_multiple_files=True)\n",
        "    if st.button(\"Enter\"):\n",
        "        with st.spinner(\"Processing\"):\n",
        "            # get pdf\n",
        "            raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "            # get the text chunks\n",
        "            text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "            # create embeddings and vector store\n",
        "            vectorstore = get_vectorstore(text_chunks)\n",
        "\n",
        "            # create conversation chain\n",
        "            st.session_state.conversation = get_conversation_chain(\n",
        "                vectorstore)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00yJ0lBbl_Ln"
      },
      "outputs": [],
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spH4RAkLmCT3",
        "outputId": "b659056e-f6db-4b10-d06c-1b38479251ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.221.40.81:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.312s\n",
            "your url is: https://curly-states-tie.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dOKpEyQ0wOB0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}